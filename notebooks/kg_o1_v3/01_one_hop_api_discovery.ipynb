{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: One-Hop API Discovery (GO/NO-GO Gate)\n",
    "\n",
    "## Critical Mission\n",
    "\n",
    "**v2 concluded KRAKEN has a \"semantic gap\" but NEVER tested the `/one-hop` endpoint.**\n",
    "\n",
    "This notebook definitively answers: **Does `/one-hop` return semantic relations?**\n",
    "\n",
    "This is a **GATE CHECK** - if `/one-hop` doesn't exist or only returns vocabulary equivalency,\n",
    "we pivot immediately rather than continuing with NB02-08.\n",
    "\n",
    "## Decision Criteria\n",
    "\n",
    "| Finding | Decision |\n",
    "|---------|----------|\n",
    "| `/one-hop` returns `participates_in`, `catalyzed_by`, etc. | **GO** - proceed with v3 |\n",
    "| `/one-hop` only returns `same_as`, `equivalent_to` | **PIVOT** - still vocabulary, not semantic |\n",
    "| `/one-hop` doesn't exist (404) | **PIVOT** - use Reactome/KEGG fallback |\n",
    "| Endpoint exists but empty results | **INVESTIGATE** - may need different entity types |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/trentleslie/Insync/projects/biomapper2\n",
      "Output directory: /home/trentleslie/Insync/projects/biomapper2/notebooks/kg_o1_v3/outputs\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import utilities\n",
    "from kg_o1_v3_utils import (\n",
    "    test_one_hop, get_predicates, parse_one_hop_edges, get_semantic_edges,\n",
    "    classify_predicate, classify_all_predicates,\n",
    "    hybrid_search, save_json, load_json,\n",
    "    TEST_ENTITIES, SEMANTIC_PREDICATES, EQUIVALENCY_PREDICATES,\n",
    ")\n",
    "from biomapper2.utils import kestrel_request\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path.cwd() / 'outputs'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test /one-hop Endpoint Existence\n",
    "\n",
    "First, let's check if the endpoint exists at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: Checking /one-hop endpoint existence\n",
      "============================================================\n",
      "\n",
      "Test entity: glucose (CHEBI:4167)\n",
      "\n",
      "Endpoint exists! Response type: dict\n",
      "Response keys: ['edge_schema', 'results', 'nodes']\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check if /one-hop endpoint exists\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 1: Checking /one-hop endpoint existence\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_entity_id, test_entity_name = TEST_ENTITIES[0]  # glucose\n",
    "print(f\"\\nTest entity: {test_entity_name} ({test_entity_id})\")\n",
    "\n",
    "one_hop_result = test_one_hop(test_entity_id, direction=\"both\")\n",
    "\n",
    "# Check for 404 error\n",
    "if isinstance(one_hop_result, dict) and one_hop_result.get('error') == 'endpoint_not_found':\n",
    "    print(\"\\n\" + \"!\" * 60)\n",
    "    print(\"CRITICAL: /one-hop endpoint does NOT exist (404)\")\n",
    "    print(\"!\" * 60)\n",
    "    endpoint_exists = False\n",
    "else:\n",
    "    print(f\"\\nEndpoint exists! Response type: {type(one_hop_result).__name__}\")\n",
    "    endpoint_exists = True\n",
    "    \n",
    "    # Show response structure\n",
    "    if isinstance(one_hop_result, dict):\n",
    "        print(f\"Response keys: {list(one_hop_result.keys())}\")\n",
    "    elif isinstance(one_hop_result, list):\n",
    "        print(f\"Response is a list with {len(one_hop_result)} items\")\n",
    "        if one_hop_result:\n",
    "            print(f\"First item keys: {list(one_hop_result[0].keys()) if isinstance(one_hop_result[0], dict) else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If endpoint doesn't exist, check alternative endpoints\n",
    "if not endpoint_exists:\n",
    "    print(\"\\nChecking alternative endpoints...\")\n",
    "    \n",
    "    alternative_endpoints = [\n",
    "        ('get-edges', {'node_id': test_entity_id}),\n",
    "        ('predicates', None),\n",
    "        ('similar-nodes', {'node_id': test_entity_id, 'limit': 5}),\n",
    "    ]\n",
    "    \n",
    "    for endpoint, payload in alternative_endpoints:\n",
    "        try:\n",
    "            if payload:\n",
    "                result = kestrel_request('POST', endpoint, json=payload)\n",
    "            else:\n",
    "                result = kestrel_request('GET', endpoint)\n",
    "            print(f\"  {endpoint}: EXISTS (type: {type(result).__name__})\")\n",
    "        except Exception as e:\n",
    "            status = getattr(e, 'response', None)\n",
    "            if status and hasattr(status, 'status_code'):\n",
    "                print(f\"  {endpoint}: {status.status_code}\")\n",
    "            else:\n",
    "                print(f\"  {endpoint}: ERROR - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Multiple Entities and Directions\n",
    "\n",
    "If the endpoint exists, test it with multiple known metabolites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 2: Testing multiple entities and directions\n",
      "============================================================\n",
      "glucose (forward): 105 edges\n",
      "glucose (reverse): 78 edges\n",
      "glucose (both): 141 edges\n",
      "NAD+ (forward): 52 edges\n",
      "NAD+ (reverse): 46 edges\n",
      "NAD+ (both): 64 edges\n",
      "cholesterol (forward): 84 edges\n",
      "cholesterol (reverse): 62 edges\n",
      "cholesterol (both): 98 edges\n",
      "alanine (forward): 105 edges\n",
      "alanine (reverse): 78 edges\n",
      "alanine (both): 141 edges\n",
      "ATP (forward): 16 edges\n",
      "ATP (reverse): 44 edges\n",
      "ATP (both): 55 edges\n",
      "water (forward): 87 edges\n",
      "water (reverse): 63 edges\n",
      "water (both): 113 edges\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multiple entities and directions\n",
    "if endpoint_exists:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 2: Testing multiple entities and directions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for entity_id, entity_name in TEST_ENTITIES:\n",
    "        for direction in [\"forward\", \"reverse\", \"both\"]:\n",
    "            result = test_one_hop(entity_id, direction=direction)\n",
    "            \n",
    "            # Use parse_one_hop_edges to properly extract edges\n",
    "            edges = parse_one_hop_edges(result)\n",
    "            \n",
    "            all_results.append({\n",
    "                'entity_id': entity_id,\n",
    "                'entity_name': entity_name,\n",
    "                'direction': direction,\n",
    "                'num_edges': len(edges),\n",
    "                'edges': edges,\n",
    "            })\n",
    "            \n",
    "            print(f\"{entity_name} ({direction}): {len(edges)} edges\")\n",
    "else:\n",
    "    all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Total edges found: 1432\n",
      "  Entities with edges: 6/6\n",
      "\n",
      "Sample edge structure from glucose:\n",
      "  subject_id: CHEBI:4167\n",
      "  predicate: biolink:mentioned_in_clinical_trials_for\n",
      "  object_id: MONDO:0004946\n",
      "  end_node_id: MONDO:0004946\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "if all_results:\n",
    "    total_edges = sum(r['num_edges'] for r in all_results)\n",
    "    entities_with_edges = len(set(r['entity_id'] for r in all_results if r['num_edges'] > 0))\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total edges found: {total_edges}\")\n",
    "    print(f\"  Entities with edges: {entities_with_edges}/{len(TEST_ENTITIES)}\")\n",
    "    \n",
    "    # Show sample edge structure (now properly parsed as dict)\n",
    "    for r in all_results:\n",
    "        if r['edges']:\n",
    "            print(f\"\\nSample edge structure from {r['entity_name']}:\")\n",
    "            edge = r['edges'][0]\n",
    "            print(f\"  subject_id: {edge.get('subject_id')}\")\n",
    "            print(f\"  predicate: {edge.get('predicate')}\")\n",
    "            print(f\"  object_id: {edge.get('object_id')}\")\n",
    "            print(f\"  end_node_id: {edge.get('end_node_id')}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Predicates (CRITICAL CHECK)\n",
    "\n",
    "This is the **most important analysis**. We need to determine if the predicates are:\n",
    "- **SEMANTIC**: `participates_in`, `catalyzes`, `treats`, etc.\n",
    "- **EQUIVALENCY**: `same_as`, `equivalent_to`, `xref`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: Predicate Analysis (CRITICAL CHECK)\n",
      "============================================================\n",
      "\n",
      "Unique predicates found: 18\n",
      "\n",
      "Predicate frequency:\n",
      "   490x  [SEMANTIC] biolink:related_to\n",
      "   359x  [SEMANTIC] biolink:subclass_of\n",
      "   129x  [SEMANTIC] biolink:has_participant\n",
      "    92x  [SEMANTIC] biolink:has_input\n",
      "    80x  [SEMANTIC] biolink:in_clinical_trials_for\n",
      "    67x  [SEMANTIC] biolink:close_match\n",
      "    50x  [SEMANTIC] biolink:has_chemical_role\n",
      "    46x  [SEMANTIC] biolink:treats\n",
      "    34x  [SEMANTIC] biolink:mentioned_in_clinical_trials_for\n",
      "    28x  [SEMANTIC] biolink:chemically_similar_to\n",
      "    18x  [SEMANTIC] biolink:has_output\n",
      "    11x  [SEMANTIC] biolink:applied_to_treat\n",
      "    10x  [SEMANTIC] biolink:has_part\n",
      "     7x  [???     ] biolink:physically_interacts_with\n",
      "     4x  [???     ] biolink:produces\n",
      "     4x  [EQUIV   ] biolink:same_as\n",
      "     2x  [SEMANTIC] biolink:affects\n",
      "     1x  [SEMANTIC] biolink:causes\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Extract and classify all predicates\n",
    "if all_results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 3: Predicate Analysis (CRITICAL CHECK)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Collect all predicates (edges are now properly parsed dicts)\n",
    "    predicate_counts = Counter()\n",
    "    predicate_examples = defaultdict(list)\n",
    "    \n",
    "    for r in all_results:\n",
    "        for edge in r['edges']:\n",
    "            pred = edge.get('predicate', 'unknown')\n",
    "            predicate_counts[pred] += 1\n",
    "            if len(predicate_examples[pred]) < 3:\n",
    "                predicate_examples[pred].append({\n",
    "                    'subject': r['entity_name'],\n",
    "                    'subject_id': edge.get('subject_id'),\n",
    "                    'object_id': edge.get('object_id', edge.get('end_node_id', 'unknown')),\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nUnique predicates found: {len(predicate_counts)}\")\n",
    "    print(\"\\nPredicate frequency:\")\n",
    "    for pred, count in predicate_counts.most_common(20):\n",
    "        classification = classify_predicate(pred)\n",
    "        marker = \"SEMANTIC\" if classification == 'semantic' else \"EQUIV\" if classification == 'equivalency' else \"???\"\n",
    "        print(f\"  {count:4d}x  [{marker:8s}] {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREDICATE CLASSIFICATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Semantic predicates: 15 types (1417 edges, 99.0%)\n",
      "  - biolink:mentioned_in_clinical_trials_for\n",
      "  - biolink:in_clinical_trials_for\n",
      "  - biolink:treats\n",
      "  - biolink:related_to\n",
      "  - biolink:applied_to_treat\n",
      "  - biolink:has_chemical_role\n",
      "  - biolink:subclass_of\n",
      "  - biolink:close_match\n",
      "  - biolink:has_output\n",
      "  - biolink:has_participant\n",
      "  - biolink:has_part\n",
      "  - biolink:has_input\n",
      "  - biolink:chemically_similar_to\n",
      "  - biolink:affects\n",
      "  - biolink:causes\n",
      "\n",
      "Equivalency predicates: 1 types (4 edges, 0.3%)\n",
      "  - biolink:same_as\n",
      "\n",
      "Unknown predicates: 2 types (11 edges, 0.8%)\n",
      "  - biolink:produces\n",
      "  - biolink:physically_interacts_with\n"
     ]
    }
   ],
   "source": [
    "# Classify all predicates\n",
    "if all_results and predicate_counts:\n",
    "    all_predicates = list(predicate_counts.keys())\n",
    "    classification = classify_all_predicates(all_predicates)\n",
    "    \n",
    "    semantic_count = sum(predicate_counts[p] for p in classification['semantic'])\n",
    "    equiv_count = sum(predicate_counts[p] for p in classification['equivalency'])\n",
    "    unknown_count = sum(predicate_counts[p] for p in classification['unknown'])\n",
    "    total_count = semantic_count + equiv_count + unknown_count\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICATE CLASSIFICATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSemantic predicates: {len(classification['semantic'])} types ({semantic_count} edges, {100*semantic_count/total_count:.1f}%)\")\n",
    "    for p in classification['semantic']:\n",
    "        print(f\"  - {p}\")\n",
    "    \n",
    "    print(f\"\\nEquivalency predicates: {len(classification['equivalency'])} types ({equiv_count} edges, {100*equiv_count/total_count:.1f}%)\")\n",
    "    for p in classification['equivalency']:\n",
    "        print(f\"  - {p}\")\n",
    "    \n",
    "    print(f\"\\nUnknown predicates: {len(classification['unknown'])} types ({unknown_count} edges, {100*unknown_count/total_count:.1f}%)\")\n",
    "    for p in classification['unknown']:\n",
    "        print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Relation Examples\n",
    "\n",
    "If semantic predicates exist, show examples of the entity-relation-entity triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEMANTIC RELATION EXAMPLES (TRUE MULTI-HOP CAPABILITY!)\n",
      "============================================================\n",
      "  glucose --[biolink:mentioned_in_clinical_trials_for]--> MONDO:0004946\n",
      "  glucose --[biolink:mentioned_in_clinical_trials_for]--> MONDO:0005148\n",
      "  glucose --[biolink:mentioned_in_clinical_trials_for]--> MONDO:0002909\n",
      "  glucose --[biolink:in_clinical_trials_for]--> MONDO:0004946\n",
      "  glucose --[biolink:in_clinical_trials_for]--> MONDO:0004946\n",
      "  glucose --[biolink:in_clinical_trials_for]--> MONDO:0005148\n",
      "  glucose --[biolink:treats]--> MONDO:0004946\n",
      "  glucose --[biolink:treats]--> MONDO:0004946\n",
      "  glucose --[biolink:treats]--> MONDO:0005148\n",
      "  glucose --[biolink:related_to]--> MONDO:0004946\n"
     ]
    }
   ],
   "source": [
    "# Show semantic relation examples\n",
    "if all_results and predicate_counts:\n",
    "    semantic_examples = []\n",
    "    \n",
    "    for p in classification.get('semantic', []):\n",
    "        if p in predicate_examples:\n",
    "            for ex in predicate_examples[p]:\n",
    "                semantic_examples.append({\n",
    "                    'subject': ex['subject'],\n",
    "                    'predicate': p,\n",
    "                    'object': ex.get('object_id', ex.get('object', 'unknown')),\n",
    "                })\n",
    "    \n",
    "    if semantic_examples:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SEMANTIC RELATION EXAMPLES (TRUE MULTI-HOP CAPABILITY!)\")\n",
    "        print(\"=\"*60)\n",
    "        for ex in semantic_examples[:10]:\n",
    "            print(f\"  {ex['subject']} --[{ex['predicate']}]--> {ex['object']}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"!\"*60)\n",
    "        print(\"NO SEMANTIC RELATIONS FOUND\")\n",
    "        print(\"!\"*60)\n",
    "        print(\"This confirms v2's finding: KRAKEN is vocabulary-focused, not semantic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test with Predicate Filters\n",
    "\n",
    "Try filtering for specific semantic predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 5: Predicate Filtering (API capability check)\n",
      "============================================================\n",
      "\n",
      "Predicate filtering by post-processing retrieved edges:\n",
      "\n",
      "glucose (CHEBI:4167):\n",
      "  biolink:treats: 10 edges\n",
      "    --> MONDO:0004946\n",
      "    --> MONDO:0004946\n",
      "  biolink:in_clinical_trials_for: 14 edges\n",
      "    --> MONDO:0004946\n",
      "    --> MONDO:0004946\n",
      "  biolink:related_to: 104 edges\n",
      "    --> MONDO:0004946\n",
      "    --> CHEBI:4167\n",
      "\n",
      "NAD+ (CHEBI:15846):\n",
      "  biolink:treats: 1 edges\n",
      "    --> UMLS:C1112459\n",
      "  biolink:in_clinical_trials_for: 3 edges\n",
      "    --> MONDO:0100233\n",
      "    --> MONDO:0100096\n",
      "  biolink:related_to: 52 edges\n",
      "    --> CHEBI:13389\n",
      "    --> CHEBI:13389\n",
      "\n",
      "cholesterol (CHEBI:16113):\n",
      "  biolink:treats: 5 edges\n",
      "    --> MONDO:0021187\n",
      "    --> MONDO:0010035\n",
      "  biolink:in_clinical_trials_for: 10 edges\n",
      "    --> HP:0003124\n",
      "    --> MONDO:0021187\n",
      "  biolink:related_to: 66 edges\n",
      "    --> HP:0003124\n",
      "    --> CHEBI:16113\n"
     ]
    }
   ],
   "source": [
    "# Test predicate filtering\n",
    "# NOTE: The API may not support predicate_filter parameter - we'll test and document\n",
    "if endpoint_exists:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 5: Predicate Filtering (API capability check)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Instead of filtering at API level (which may not be supported),\n",
    "    # we filter the edges we already retrieved\n",
    "    print(\"\\nPredicate filtering by post-processing retrieved edges:\")\n",
    "    \n",
    "    target_predicates = [\n",
    "        'biolink:participates_in',\n",
    "        'biolink:treats', \n",
    "        'biolink:in_clinical_trials_for',\n",
    "        'biolink:associated_with',\n",
    "        'biolink:related_to',\n",
    "    ]\n",
    "    \n",
    "    for entity_id, entity_name in TEST_ENTITIES[:3]:\n",
    "        print(f\"\\n{entity_name} ({entity_id}):\")\n",
    "        \n",
    "        # Get all edges first\n",
    "        result = test_one_hop(entity_id, direction=\"both\", limit=50)\n",
    "        all_edges = parse_one_hop_edges(result)\n",
    "        \n",
    "        for target_pred in target_predicates:\n",
    "            matching_edges = [e for e in all_edges if e.get('predicate') == target_pred]\n",
    "            if matching_edges:\n",
    "                print(f\"  {target_pred}: {len(matching_edges)} edges\")\n",
    "                for e in matching_edges[:2]:\n",
    "                    obj_id = e.get('object_id', e.get('end_node_id', '?'))\n",
    "                    print(f\"    --> {obj_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Category Filtering\n",
    "\n",
    "Try filtering for specific end categories like Pathway, Disease, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 6: Category Analysis (from edges)\n",
      "============================================================\n",
      "\n",
      "End node category distribution (by ID prefix):\n",
      "  CHEBI: 766 edges\n",
      "    glucose --[biolink:has_chemical_role]--> CHEBI:78675\n",
      "    glucose --[biolink:related_to]--> CHEBI:78675\n",
      "  MONDO: 225 edges\n",
      "    glucose --[biolink:mentioned_in_clinical_trials_for]--> MONDO:0004946\n",
      "    glucose --[biolink:in_clinical_trials_for]--> MONDO:0004946\n",
      "  GO: 184 edges\n",
      "    glucose --[biolink:has_output]--> GO:0006094\n",
      "    glucose --[biolink:has_output]--> GO:0006094\n",
      "  UMLS: 95 edges\n",
      "    glucose --[biolink:related_to]--> UMLS:C4082776\n",
      "    glucose --[biolink:related_to]--> UMLS:C4082776\n",
      "  SMPDB: 66 edges\n",
      "    NAD+ --[biolink:has_participant]--> SMPDB:SMP0017930\n",
      "    NAD+ --[biolink:has_participant]--> SMPDB:SMP0017930\n",
      "  MESH: 31 edges\n",
      "    glucose --[biolink:subclass_of]--> MESH:D000429\n",
      "    glucose --[biolink:subclass_of]--> MESH:D000429\n",
      "  HP: 22 edges\n",
      "    cholesterol --[biolink:related_to]--> HP:0003124\n",
      "    cholesterol --[biolink:related_to]--> HP:0003124\n",
      "  PathWhiz: 12 edges\n",
      "    NAD+ --[biolink:has_participant]--> PathWhiz:PW018813\n",
      "    NAD+ --[biolink:has_participant]--> PathWhiz:PW018813\n",
      "  NCBIGene: 9 edges\n",
      "    NAD+ --[biolink:affects]--> NCBIGene:7226\n",
      "    NAD+ --[biolink:related_to]--> NCBIGene:7226\n",
      "  CHV: 8 edges\n",
      "    glucose --[biolink:close_match]--> CHV:0000005565\n",
      "    glucose --[biolink:close_match]--> CHV:0000005565\n",
      "  UNII: 6 edges\n",
      "    glucose --[biolink:chemically_similar_to]--> UNII:9G2MP84A8W\n",
      "    glucose --[biolink:chemically_similar_to]--> UNII:9G2MP84A8W\n",
      "  NBO: 3 edges\n",
      "    water --[biolink:has_participant]--> NBO:0000132\n",
      "    water --[biolink:has_participant]--> NBO:0000132\n",
      "  PUBCHEM.COMPOUND: 2 edges\n",
      "    NAD+ --[biolink:related_to]--> PUBCHEM.COMPOUND:74762\n",
      "    NAD+ --[biolink:related_to]--> PUBCHEM.COMPOUND:74762\n",
      "  UniProtKB: 2 edges\n",
      "    ATP --[biolink:physically_interacts_with]--> UniProtKB:D4N3P2\n",
      "    ATP --[biolink:physically_interacts_with]--> UniProtKB:D4N3P2\n",
      "  BFO: 1 edges\n",
      "    ATP --[biolink:subclass_of]--> BFO:0000004\n"
     ]
    }
   ],
   "source": [
    "# Test category filtering\n",
    "# Similar to predicate filtering, we may need to post-process rather than API-filter\n",
    "if endpoint_exists:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 6: Category Analysis (from edges)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze categories from the edges we've collected\n",
    "    category_counts = Counter()\n",
    "    category_examples = defaultdict(list)\n",
    "    \n",
    "    for r in all_results:\n",
    "        for edge in r['edges']:\n",
    "            # The end_node_id can tell us category info\n",
    "            end_node = edge.get('end_node_id', edge.get('object_id', ''))\n",
    "            \n",
    "            # Extract category from node ID prefix\n",
    "            if ':' in end_node:\n",
    "                prefix = end_node.split(':')[0]\n",
    "                category_counts[prefix] += 1\n",
    "                if len(category_examples[prefix]) < 3:\n",
    "                    category_examples[prefix].append({\n",
    "                        'subject': r['entity_name'],\n",
    "                        'predicate': edge.get('predicate', ''),\n",
    "                        'object_id': end_node,\n",
    "                    })\n",
    "    \n",
    "    print(\"\\nEnd node category distribution (by ID prefix):\")\n",
    "    for cat, count in category_counts.most_common(15):\n",
    "        print(f\"  {cat}: {count} edges\")\n",
    "        for ex in category_examples[cat][:2]:\n",
    "            print(f\"    {ex['subject']} --[{ex['predicate']}]--> {ex['object_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check /predicates Endpoint\n",
    "\n",
    "Get the full list of available predicates in KRAKEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 7: /predicates Endpoint\n",
      "============================================================\n",
      "\n",
      "Total predicates from API: 87\n",
      "\n",
      "Classification:\n",
      "  Semantic: 25\n",
      "    - biolink:actively_involved_in\n",
      "    - biolink:affects\n",
      "    - biolink:applied_to_treat\n",
      "    - biolink:associated_with\n",
      "    - biolink:capable_of\n",
      "    - biolink:catalyzes\n",
      "    - biolink:causes\n",
      "    - biolink:chemically_similar_to\n",
      "    - biolink:close_match\n",
      "    - biolink:coexists_with\n",
      "    - biolink:contributes_to\n",
      "    - biolink:correlated_with\n",
      "    - biolink:expressed_in\n",
      "    - biolink:has_chemical_role\n",
      "    - biolink:has_input\n",
      "    - biolink:has_metabolite\n",
      "    - biolink:has_output\n",
      "    - biolink:has_part\n",
      "    - biolink:has_participant\n",
      "    - biolink:in_clinical_trials_for\n",
      "    - biolink:located_in\n",
      "    - biolink:participates_in\n",
      "    - biolink:related_to\n",
      "    - biolink:subclass_of\n",
      "    - biolink:treats\n",
      "\n",
      "  Equivalency: 1\n",
      "    - biolink:same_as\n",
      "\n",
      "  Unknown: 61\n",
      "    - biolink:affects_likelihood_of\n",
      "    - biolink:affects_response_to\n",
      "    - biolink:ameliorates_condition\n",
      "    - biolink:assesses\n",
      "    - biolink:associated_with_resistance_to\n",
      "    - biolink:beneficial_in_models_for\n",
      "    - biolink:binds\n",
      "    - biolink:biomarker_for\n",
      "    - biolink:broad_match\n",
      "    - biolink:colocalizes_with\n",
      "    ... and 51 more\n"
     ]
    }
   ],
   "source": [
    "# Check /predicates endpoint\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 7: /predicates Endpoint\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_predicates_from_api = get_predicates()\n",
    "\n",
    "if all_predicates_from_api:\n",
    "    print(f\"\\nTotal predicates from API: {len(all_predicates_from_api)}\")\n",
    "    \n",
    "    api_classification = classify_all_predicates(all_predicates_from_api)\n",
    "    \n",
    "    print(f\"\\nClassification:\")\n",
    "    print(f\"  Semantic: {len(api_classification['semantic'])}\")\n",
    "    for p in api_classification['semantic']:\n",
    "        print(f\"    - {p}\")\n",
    "    \n",
    "    print(f\"\\n  Equivalency: {len(api_classification['equivalency'])}\")\n",
    "    for p in api_classification['equivalency']:\n",
    "        print(f\"    - {p}\")\n",
    "    \n",
    "    print(f\"\\n  Unknown: {len(api_classification['unknown'])}\")\n",
    "    for p in api_classification['unknown'][:10]:  # Show first 10\n",
    "        print(f\"    - {p}\")\n",
    "    if len(api_classification['unknown']) > 10:\n",
    "        print(f\"    ... and {len(api_classification['unknown']) - 10} more\")\n",
    "else:\n",
    "    print(\"\\n/predicates endpoint not available or returned empty results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GO/NO-GO Decision\n",
    "\n",
    "Based on all the tests, make the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GO/NO-GO DECISION\n",
      "============================================================\n",
      "\n",
      "========================================\n",
      "DECISION: GO\n",
      "========================================\n",
      "\n",
      "Reasoning: Found 15 semantic predicates with 99.0% semantic edges. Proceed with v3.\n",
      "\n",
      "Key metrics:\n",
      "  - Endpoint exists: True\n",
      "  - Total edges: 1432\n",
      "  - Semantic predicates: 15\n",
      "  - Semantic edge %: 99.0%\n"
     ]
    }
   ],
   "source": [
    "# GO/NO-GO Decision\n",
    "print(\"=\"*60)\n",
    "print(\"GO/NO-GO DECISION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "decision_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'endpoint_exists': endpoint_exists,\n",
    "    'total_edges_found': sum(r['num_edges'] for r in all_results) if all_results else 0,\n",
    "    'unique_predicates': len(predicate_counts) if 'predicate_counts' in dir() and predicate_counts else 0,\n",
    "    'semantic_predicate_count': len(classification.get('semantic', [])) if 'classification' in dir() else 0,\n",
    "    'equivalency_predicate_count': len(classification.get('equivalency', [])) if 'classification' in dir() else 0,\n",
    "    'semantic_edge_percent': 0,\n",
    "    'decision': None,\n",
    "    'reasoning': None,\n",
    "}\n",
    "\n",
    "# Calculate semantic percentage\n",
    "if 'classification' in dir() and 'predicate_counts' in dir():\n",
    "    semantic_count = sum(predicate_counts.get(p, 0) for p in classification.get('semantic', []))\n",
    "    total_count = sum(predicate_counts.values())\n",
    "    if total_count > 0:\n",
    "        decision_data['semantic_edge_percent'] = 100 * semantic_count / total_count\n",
    "\n",
    "# Make decision\n",
    "if not endpoint_exists:\n",
    "    decision_data['decision'] = 'PIVOT'\n",
    "    decision_data['reasoning'] = '/one-hop endpoint does not exist (404). Must use Reactome/KEGG fallback.'\n",
    "elif decision_data['total_edges_found'] == 0:\n",
    "    decision_data['decision'] = 'INVESTIGATE'\n",
    "    decision_data['reasoning'] = 'Endpoint exists but returned no edges. May need different entity types or parameters.'\n",
    "elif decision_data['semantic_predicate_count'] == 0:\n",
    "    decision_data['decision'] = 'PIVOT'\n",
    "    decision_data['reasoning'] = 'No semantic predicates found. Confirms v2: KRAKEN is vocabulary-focused only.'\n",
    "elif decision_data['semantic_edge_percent'] < 10:\n",
    "    decision_data['decision'] = 'PIVOT'\n",
    "    decision_data['reasoning'] = f'Only {decision_data[\"semantic_edge_percent\"]:.1f}% semantic edges. Insufficient for multi-hop reasoning.'\n",
    "else:\n",
    "    decision_data['decision'] = 'GO'\n",
    "    decision_data['reasoning'] = f'Found {decision_data[\"semantic_predicate_count\"]} semantic predicates with {decision_data[\"semantic_edge_percent\"]:.1f}% semantic edges. Proceed with v3.'\n",
    "\n",
    "# Display decision\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"DECISION: {decision_data['decision']}\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"\\nReasoning: {decision_data['reasoning']}\")\n",
    "print(f\"\\nKey metrics:\")\n",
    "print(f\"  - Endpoint exists: {decision_data['endpoint_exists']}\")\n",
    "print(f\"  - Total edges: {decision_data['total_edges_found']}\")\n",
    "print(f\"  - Semantic predicates: {decision_data['semantic_predicate_count']}\")\n",
    "print(f\"  - Semantic edge %: {decision_data['semantic_edge_percent']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Audit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audit results saved to: /home/trentleslie/Insync/projects/biomapper2/notebooks/kg_o1_v3/outputs/one_hop_api_audit.json\n"
     ]
    }
   ],
   "source": [
    "# Save audit results\n",
    "audit_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'go_no_go_decision': decision_data,\n",
    "    'endpoint_tests': {\n",
    "        'one_hop_exists': endpoint_exists,\n",
    "        'test_entities': TEST_ENTITIES,\n",
    "        'results_per_entity': [\n",
    "            {\n",
    "                'entity_id': r['entity_id'],\n",
    "                'entity_name': r['entity_name'],\n",
    "                'direction': r['direction'],\n",
    "                'num_edges': r['num_edges'],\n",
    "            }\n",
    "            for r in all_results\n",
    "        ] if all_results else [],\n",
    "    },\n",
    "    'predicate_analysis': {\n",
    "        'unique_predicates': list(predicate_counts.keys()) if 'predicate_counts' in dir() and predicate_counts else [],\n",
    "        'predicate_counts': dict(predicate_counts) if 'predicate_counts' in dir() and predicate_counts else {},\n",
    "        'classification': classification if 'classification' in dir() else {},\n",
    "        'predicate_examples': dict(predicate_examples) if 'predicate_examples' in dir() else {},\n",
    "    },\n",
    "    'api_predicates': all_predicates_from_api if 'all_predicates_from_api' in dir() else [],\n",
    "}\n",
    "\n",
    "save_json(audit_data, OUTPUT_DIR / 'one_hop_api_audit.json')\n",
    "print(f\"\\nAudit results saved to: {OUTPUT_DIR / 'one_hop_api_audit.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the `/one-hop` endpoint to determine if KRAKEN supports true semantic graph traversal.\n",
    "\n",
    "**Key findings are saved to `outputs/one_hop_api_audit.json`.**\n",
    "\n",
    "### Next Steps Based on Decision:\n",
    "\n",
    "- **GO**: Proceed to NB02 (Predicate Relationship Mapping)\n",
    "- **PIVOT (no semantic relations)**: Document finding, v3 not viable with current KRAKEN\n",
    "- **INVESTIGATE**: Try different entity types or parameters before deciding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NOTEBOOK 01 COMPLETE\n",
      "============================================================\n",
      "\n",
      "Decision: GO\n",
      "\n",
      "Next step: Proceed to NB02: Predicate & Relationship Mapping\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 01 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDecision: {decision_data['decision']}\")\n",
    "print(f\"\\nNext step: \", end=\"\")\n",
    "\n",
    "if decision_data['decision'] == 'GO':\n",
    "    print(\"Proceed to NB02: Predicate & Relationship Mapping\")\n",
    "elif decision_data['decision'] == 'PIVOT':\n",
    "    print(\"Document finding. Consider Reactome/KEGG fallback approach.\")\n",
    "else:\n",
    "    print(\"Investigate further with different entity types or parameters.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomapper2 (uv)",
   "language": "python",
   "name": "biomapper2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
